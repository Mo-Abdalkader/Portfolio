<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Day 2 â€” The ML Pipeline | 30-Day AI Ramadan Series</title>
  <meta name="description" content="The complete journey from raw data to a deployed ML model â€” data collection, cleaning, feature engineering, training, evaluation, and deployment explained step by step." />
  <link rel="icon" type="image/png" href="../assets/logo.png" />
  <script>(function(){var t=localStorage.getItem('theme')||'dark';document.documentElement.setAttribute('data-theme',t);})();</script>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-6JXJJ6Y1ZC"></script>
  <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js',new Date());gtag('config','G-6JXJJ6Y1ZC');</script>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Syne:wght@400;700;800&family=JetBrains+Mono:wght@300;400;500&family=Outfit:wght@300;400;500;600&family=Playfair+Display:ital,wght@0,700;1,400&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="../css/style.css" />
</head>
<body class="blog-post-page">

<div class="reading-progress" id="readingProgress"></div>

<div class="announcement-banner ramadan-banner" id="ramadanBanner" style="display:none">
  <div class="ramadan-stars" aria-hidden="true"><span>âœ¦</span><span>âœ§</span><span>âœ¦</span><span>âœ§</span></div>
  <div class="ramadan-content">
    <span class="ramadan-crescent">â˜½</span>
    <p>ğŸŒ™ Ramadan Mubarak! Follow the 30-Day AI Series â€” one concept every day.</p>
    <a href="../blog.html" class="ramadan-cta">Explore Series â†’</a>
  </div>
  <button class="ramadan-close" onclick="this.closest('.announcement-banner').style.display='none';window.dispatchEvent(new Event('resize'))" aria-label="Dismiss">Ã—</button>
</div>
<div class="announcement-banner eid-banner" id="eidBanner" style="display:none">
  <div class="eid-content">
    <p>ğŸŒ™ Eid Mubarak! ğŸŒ™ &nbsp;â€”&nbsp; Wishing you joy, peace, and blessings this Eid.</p>
  </div>
  <button class="eid-close" onclick="this.closest('.announcement-banner').style.display='none';window.dispatchEvent(new Event('resize'))" aria-label="Dismiss">Ã—</button>
</div>

<nav id="navbar" class="navbar scrolled" role="navigation" aria-label="Main navigation">
  <div class="nav-inner">
    <a href="../index.html" class="nav-logo" aria-label="Home"><img src="../assets/logo.png" alt="Mohamed Abdalkader logo" /></a>
    <ul class="nav-links" role="list">
      <li><a href="../index.html" class="nav-link">Home</a></li>
      <li><a href="../projects.html" class="nav-link">Projects</a></li>
      <li><a href="../blog.html" class="nav-link active">Blog</a></li>
      <li><a href="../index.html#contact" class="nav-link">Contact</a></li>
    </ul>
    <div class="nav-right">
      <button class="theme-toggle" id="themeToggle" onclick="toggleTheme()" aria-label="Toggle theme">â˜€ï¸</button>
      <button class="hamburger" id="hamburger" aria-expanded="false" aria-label="Open menu"><span></span><span></span><span></span></button>
    </div>
  </div>
</nav>
<div class="mobile-menu" id="mobileMenu" role="menu">
  <a href="../index.html" class="nav-link" role="menuitem">Home</a>
  <a href="../projects.html" class="nav-link" role="menuitem">Projects</a>
  <a href="../blog.html" class="nav-link" role="menuitem">Blog</a>
  <a href="../index.html#contact" class="nav-link" role="menuitem">Contact</a>
</div>

<header class="post-hero">
  <div class="dot-grid" aria-hidden="true"></div>
  <div class="post-hero-inner">
    <div class="post-series-badge">ğŸŒ™ 30-Day AI Ramadan Series &nbsp;Â·&nbsp; Day 2 of 30</div>
    <div class="post-hero-meta">
      <span class="category-pill">Practical Guide</span>
      <span class="meta-sep">Â·</span>
      <span>Feb 20, 2026</span>
      <span class="meta-sep">Â·</span>
      <span id="readTimeCalc">15 min read</span>
      <span class="meta-sep">Â·</span>
      <span>Foundations</span>
    </div>
    <h1>The ML Pipeline:<br>From Raw Data to Working Model</h1>
    <p class="post-hero-lead">Every ML project â€” from a spam filter to a medical AI â€” follows the same underlying journey. Understanding this pipeline is understanding how ML actually gets done in the real world.</p>
    <div class="post-hero-actions">
      <a href="../blog.html" class="btn btn-ghost btn-sm">â† Back to Blog</a>
      <a href="categories.html" class="btn btn-ghost btn-sm">â˜° Categories</a>
      <button class="copy-link-btn" onclick="copyPostLink(this)">â˜ Copy Link</button>
    </div>
  </div>
</header>

<article class="post-article">
  <div class="prose">

    <blockquote>
      <p>A chef doesn't walk into a kitchen and immediately serve a dish.<br>
      They source ingredients, clean them, prep them, cook them, taste and adjust,<br>
      then plate and serve. If any step is skipped, the dish fails â€” regardless<br>
      of how talented the chef is.<br><br>
      Machine learning has the exact same workflow. It's called the ML pipeline.</p>
    </blockquote>

    <div class="pipeline-diagram" role="img" aria-label="ML Pipeline: 7 stages from Problem Definition to Deployment">
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">1</div><div class="pipeline-node-label">Problem Definition</div><div class="pipeline-node-analogy">Decide what dish you're making</div></div></div>
      <div class="pipeline-connector"></div>
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">2</div><div class="pipeline-node-label">Data Collection</div><div class="pipeline-node-analogy">Source your ingredients</div></div></div>
      <div class="pipeline-connector"></div>
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">3</div><div class="pipeline-node-label">Data Cleaning &amp; EDA</div><div class="pipeline-node-analogy">Wash, inspect, understand</div></div></div>
      <div class="pipeline-connector"></div>
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">4</div><div class="pipeline-node-label">Feature Engineering</div><div class="pipeline-node-analogy">Prep &amp; transform for cooking</div></div></div>
      <div class="pipeline-connector"></div>
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">5</div><div class="pipeline-node-label">Train / Val / Test Split</div><div class="pipeline-node-analogy">Practice, tune, then sit the real exam</div></div></div>
      <div class="pipeline-connector"></div>
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">6</div><div class="pipeline-node-label">Model Training &amp; Evaluation</div><div class="pipeline-node-analogy">Cook, taste, adjust</div></div></div>
      <div class="pipeline-connector"></div>
      <div class="pipeline-row"><div class="pipeline-node"><div class="pipeline-node-num">7</div><div class="pipeline-node-label">Deployment &amp; Monitoring</div><div class="pipeline-node-analogy">Serve â€” and keep an eye on quality</div></div></div>
    </div>

    <h2>01 â€” Why the Pipeline Matters More Than the Model</h2>
    <p>New practitioners obsess over model choice: <em>should I use a Random Forest or XGBoost? Should I try a neural network?</em> Experienced engineers know a different truth: <strong>the model is almost never the bottleneck</strong>.</p>
    <p>In practice, 80% of a data scientist's time is spent on data â€” finding it, cleaning it, understanding it, transforming it. A simple model running on clean, well-prepared data almost always outperforms a sophisticated model running on messy, raw data.</p>
    <p>Understanding the pipeline is understanding where ML projects actually succeed or fail.</p>

    <div class="prose-callout gold">
      <span class="prose-callout-label">ğŸ—ï¸ Analogy</span>
      <p>Think of building a house. You could hire the world's best architect â€” but if the foundation is poorly laid and the plumbing runs backwards, the beautiful design is worthless. <strong>Data is your foundation. The model is the architecture.</strong> Skip the foundation and the house falls.</p>
    </div>

    <h2>02 â€” The Seven Stages</h2>
    <div class="prose-steps">

      <div class="prose-step">
        <div class="prose-step-num">1</div>
        <div class="prose-step-text">
          <strong>Problem Definition</strong> <span class="time-badge">â± 5â€“15% of project</span><br>
          Before touching any data or code, be ruthlessly clear: <em>What decision do you want the model to make?</em> Is it predicting a number (regression) or assigning a category (classification)? What does success look like â€” accuracy? Revenue? Lives saved? A vague problem always produces a useless model.
          <div class="step-pitfall">Common pitfall: Teams jump straight to collecting data without agreeing on what "success" means. The result is months of work solving the wrong problem entirely.</div>
        </div>
      </div>

      <div class="prose-step">
        <div class="prose-step-num">2</div>
        <div class="prose-step-text">
          <strong>Data Collection</strong> <span class="time-badge">â± 10â€“20% of project</span><br>
          Gather raw data relevant to the problem. Sources include databases, APIs, web scraping, sensors, or human annotation. Quantity and quality both matter â€” but quality wins. One thousand accurate, representative examples beat ten thousand noisy, biased ones every time.
          <div class="step-pitfall">Common pitfall: Collecting data from a convenient but unrepresentative source â€” such as only hospital records from wealthy regions. The model learns a skewed version of the world and silently fails on everyone else.</div>
        </div>
      </div>

      <div class="prose-step">
        <div class="prose-step-num">3</div>
        <div class="prose-step-text">
          <strong>Data Cleaning &amp; Exploration (EDA)</strong> <span class="time-badge">â± 30â€“60% of project</span><br>
          Real-world data is messy: missing values, duplicate rows, typos, impossible entries (a person aged 847), inconsistent formatting. <strong>Exploratory Data Analysis (EDA)</strong> means plotting distributions, finding correlations, and spotting outliers so you understand what you're actually working with before you model it.
          <div class="step-pitfall">Common pitfall: This is the most time-consuming and the most skipped stage. Models trained on uncleaned data develop mysterious failures that are nearly impossible to debug â€” because the bug is in the data, not the code.</div>
        </div>
      </div>

      <div class="prose-step">
        <div class="prose-step-num">4</div>
        <div class="prose-step-text">
          <strong>Feature Engineering</strong> <span class="time-badge">â± 10â€“30% of project</span><br>
          Raw data rarely feeds directly into a model. You transform, combine, and create <strong>features</strong> â€” the input variables the model learns from. For example: turning a birth date into "age," encoding a city name as a number, or extracting the day-of-week from a timestamp. This is where domain expertise becomes a genuine superpower.
          <div class="step-pitfall">Common pitfall: Poor features put a ceiling on performance that no model can break. A simple model with excellent features will consistently outperform an advanced model given poor ones.</div>
        </div>
      </div>

      <div class="prose-step">
        <div class="prose-step-num">5</div>
        <div class="prose-step-text">
          <strong>Train / Validation / Test Split</strong> <span class="time-badge">â± ~5% of project</span><br>
          Your data is divided into three non-overlapping buckets. The <em>training set</em> is what the model learns from. The <em>validation set</em> is used to tune the model's settings â€” its <strong>hyperparameters</strong> (configuration choices set before training, like how deep a decision tree grows or how fast it learns). The <em>test set</em> is held back until the very end to give a final, honest estimate of real-world performance.
          <div class="step-pitfall">Common pitfall: Using only a two-way split (train/test) and making tuning decisions based on the test set. Every peek at the test set leaks a small amount of information â€” inflating your accuracy estimate until the model meets real users and disappoints.</div>
        </div>
      </div>

      <div class="prose-step">
        <div class="prose-step-num">6</div>
        <div class="prose-step-text">
          <strong>Model Training &amp; Evaluation</strong> <span class="time-badge">â± 10â€“20% of project</span><br>
          Choose a model, train it on the training set, then evaluate it on the validation set using metrics suited to the problem â€” <em>accuracy</em> for balanced categories, <em>F1 score</em> for imbalanced ones (like fraud or disease), <em>RMSE</em> for numeric predictions. Tune the hyperparameters and repeat until the results are satisfying.
          <div class="step-pitfall">Common pitfall: Picking a metric that doesn't match the real goal. A model that's 99% accurate at detecting fraud sounds impressive â€” until you realise that 99% of transactions are legitimate, and simply predicting "not fraud" every time achieves the same score while missing every actual fraud.</div>
        </div>
      </div>

      <div class="prose-step">
        <div class="prose-step-num">7</div>
        <div class="prose-step-text">
          <strong>Deployment &amp; Monitoring</strong> <span class="time-badge">â± ongoing</span><br>
          The trained model is packaged (often with Docker), exposed via an API (Flask, FastAPI), and integrated into a product. But this isn't the end. Models degrade as the world changes.<br>
          <span class="term">Data drift</span> â€” the inputs your model receives start to differ from what it was trained on.<br>
          <span class="term">Model drift</span> â€” its predictions become less accurate as a result.<br>
          Both require ongoing monitoring.
          <div class="step-pitfall">Common pitfall: Teams deploy and disappear. A model trained on 2022 shopping behaviour, still running in 2025, will silently fail as user habits change â€” and nobody will know why conversions dropped.</div>
        </div>
      </div>

    </div>

    <h2>03 â€” The Critical Concept: Data Leakage</h2>
    <p><strong>Data leakage</strong> is accidentally letting the model see information it wouldn't have access to at prediction time. It makes your model look brilliant during evaluation â€” and fail completely once deployed.</p>

    <div class="prose-callout gold">
      <span class="prose-callout-label">âš ï¸ Classic Example</span>
      <p>Imagine you're building a model to predict whether a student will pass their final exam â€” based only on information available at the start of the semester. If you accidentally include their final exam score as one of the input features, the model achieves 99% accuracy in testing, because the answer is literally sitting in the data. In the real world, that score doesn't exist yet when you need to make the prediction â€” so the model is completely useless.</p>
      <p style="margin-top:0.75rem"><strong>The rule:</strong> your model must only see information it would realistically have at the exact moment it needs to make a prediction.</p>
    </div>

    <h2>04 â€” A Correct Pipeline in Python</h2>
    <p>Here's a complete, properly structured pipeline on a real medical dataset â€” 569 tumors classified as benign or malignant. Every line reflects something discussed above.</p>
    <pre><code><span class="t-key">from</span> sklearn.datasets <span class="t-key">import</span> load_breast_cancer
<span class="t-key">from</span> sklearn.model_selection <span class="t-key">import</span> train_test_split
<span class="t-key">from</span> sklearn.preprocessing <span class="t-key">import</span> StandardScaler
<span class="t-key">from</span> sklearn.linear_model <span class="t-key">import</span> LogisticRegression
<span class="t-key">from</span> sklearn.metrics <span class="t-key">import</span> classification_report

<span class="t-cmt"># Stage 2 â€” Load data (569 real tumor records: benign vs malignant)</span>
X, y = load_breast_cancer(return_X_y=<span class="t-key">True</span>)

<span class="t-cmt"># Stage 5 â€” Three-way split: 60% train | 20% validation | 20% test</span>
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=<span class="t-num">0.4</span>, random_state=<span class="t-num">42</span>, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=<span class="t-num">0.5</span>, random_state=<span class="t-num">42</span>, stratify=y_temp
)
<span class="t-cmt"># â†’ 341 train | 114 validation | 114 test</span>

<span class="t-cmt"># Stage 4 â€” Feature engineering: scale to zero mean, unit variance</span>
<span class="t-cmt"># CRITICAL: fit only on training data â€” never expose test statistics here</span>
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  <span class="t-cmt"># learn mean & std from training data only</span>
X_val   = scaler.transform(X_val)        <span class="t-cmt"># apply same scale â€” no peeking at test</span>
X_test  = scaler.transform(X_test)       <span class="t-cmt"># apply same scale â€” no peeking at test</span>

<span class="t-cmt"># Stage 6 â€” Train the model</span>
model = LogisticRegression(max_iter=<span class="t-num">200</span>)
model.fit(X_train, y_train)

<span class="t-cmt"># Stage 6 â€” Tune using validation set (adjust hyperparameters here, repeat)</span>
val_score = model.score(X_val, y_val)
<span class="t-cmt"># â†’ ~97% â€” if satisfied, proceed; otherwise adjust max_iter, C, etc.</span>

<span class="t-cmt"># Stage 6 â€” Final honest evaluation on the test set (touch only once)</span>
print(classification_report(y_test, model.predict(X_test)))</code></pre>

    <p><em>Notice: the scaler is <code>fit</code> only on training data, then <code>transform</code>ed onto everything else. Fitting on the full dataset before splitting would leak test statistics into the scaler â€” a subtle but important form of data leakage.</em></p>

    <div class="prose-callout" style="margin-top:1.5rem">
      <span class="prose-callout-label">ğŸ“Š Choosing the right metric</span>
      <table class="metrics-table">
        <thead><tr><th>Metric</th><th>What it measures</th><th>Best used whenâ€¦</th></tr></thead>
        <tbody>
          <tr><td>Accuracy</td><td>% of all predictions that are correct</td><td>Classes are roughly balanced in size</td></tr>
          <tr><td>F1 Score</td><td>Balance between catching true cases (recall) and avoiding false alarms (precision)</td><td>One class is rare â€” fraud, disease, spam</td></tr>
          <tr><td>RMSE</td><td>Typical prediction error in the original units (e.g. Â±$4,000 for house prices)</td><td>Predicting a continuous number, not a category</td></tr>
        </tbody>
      </table>
    </div>

    <p style="margin-top:0.75rem;font-size:0.875rem;color:var(--text-muted)">The code above produces output like this â€” read it line by line:</p>
    <div class="code-output"><div class="code-output-label">â†’ sample output from classification_report</div>              precision    recall  f1-score   support

   malignant       0.97      0.96      0.96        43
      benign       0.98      0.99      0.98        71

    accuracy                           0.97       114
   macro avg       0.97      0.97      0.97       114
weighted avg       0.97      0.97      0.97       114

# precision  â†’ when it predicted "malignant", it was right 97% of the time
# recall     â†’ it correctly identified 96% of all actual malignant cases
# f1-score   â†’ the single number that balances both (use this for imbalanced data)
# support    â†’ how many real examples of each class were in the test set</div>

    <h2>05 â€” Why This Matters in Production</h2>
    <p>Three principles that separate engineers who ship reliable ML from those who don't:</p>
    <ul>
      <li><strong>The pipeline is the product.</strong> In production, you're not shipping a model â€” you're shipping the entire pipeline: preprocessing, model, post-processing. If any step breaks or drifts, the product breaks.</li>
      <li><strong>Reproducibility is non-negotiable.</strong> Set random seeds. Version your data. Log your hyperparameters. If you can't reproduce your results, you can't debug them â€” and you can't trust them.</li>
      <li><strong>The test set is sacred.</strong> Look at it once, at the very end. Every time you peek at it to make a decision, you're leaking information and your final accuracy number becomes a lie.</li>
    </ul>

    <div class="prose-callout accent">
      <span class="prose-callout-label">ğŸ”— Connection to Modern AI</span>
      <p>Training GPT-4 follows this exact pipeline â€” just at a scale requiring thousands of GPUs and months of continuous compute. The data cleaning step alone (filtering the entire internet for quality text) is one of the most important and least-discussed reasons why modern LLMs work well. The pipeline is universal. Only the scale changes. The principles never do.</p>
    </div>

    <div class="knowledge-check">
      <div class="knowledge-check-header">ğŸ§  Check Your Understanding</div>
      <div class="knowledge-check-body">
        <div class="quiz-item">
          <div class="quiz-question">1. Why can't you fit the StandardScaler on the full dataset before splitting?</div>
          <button class="quiz-reveal-btn" onclick="toggleQuiz(this)">Reveal answer â†“</button>
          <div class="quiz-answer">Fitting on the full dataset means the scaler computes its mean and standard deviation using test data too. When you later evaluate on the test set, those statistics have already "seen" it â€” a form of data leakage. Always fit only on training data, then call <code>transform()</code> on everything else. The test set must behave like completely unknown future data.</div>
        </div>
        <div class="quiz-item">
          <div class="quiz-question">2. A fraud detection model achieves 99% accuracy. Should you be impressed?</div>
          <button class="quiz-reveal-btn" onclick="toggleQuiz(this)">Reveal answer â†“</button>
          <div class="quiz-answer">Almost certainly not. If only 1% of transactions are fraudulent, a model that predicts "not fraud" for everything achieves 99% accuracy while catching zero fraud. Accuracy is the wrong metric here. Use F1 score, precision/recall, or AUC-ROC instead â€” metrics that explicitly account for the imbalance between classes.</div>
        </div>
        <div class="quiz-item">
          <div class="quiz-question">3. Which stage typically takes the most time in a real project?</div>
          <button class="quiz-reveal-btn" onclick="toggleQuiz(this)">Reveal answer â†“</button>
          <div class="quiz-answer">Data Cleaning &amp; EDA (Stage 3) â€” often consuming 30â€“60% of total project time. Most newcomers expect model training to dominate, but experienced practitioners know that getting clean, well-understood data is where the real effort lives. The model itself is often the quickest part.</div>
        </div>
      </div>
    </div>

    <div class="quick-recap">
      <div class="quick-recap-header">âš¡ Quick Recap â€” The 7 Stages</div>
      <ol>
        <li><strong>Problem Definition</strong> â€” Clarify what the model must decide, and what success actually looks like.</li>
        <li><strong>Data Collection</strong> â€” Gather high-quality, representative data from databases, APIs, sensors, or annotations.</li>
        <li><strong>Data Cleaning &amp; EDA</strong> â€” Fix errors, handle missing values, explore distributions. This takes most of your time.</li>
        <li><strong>Feature Engineering</strong> â€” Transform raw data into the meaningful inputs a model can learn from.</li>
        <li><strong>Train / Val / Test Split</strong> â€” Three separate buckets: learn, tune, then evaluate honestly â€” in that order.</li>
        <li><strong>Model Training &amp; Evaluation</strong> â€” Train the model, measure with the right metric, tune hyperparameters, repeat.</li>
        <li><strong>Deployment &amp; Monitoring</strong> â€” Ship it, then watch it continuously. Models degrade silently in the real world.</li>
      </ol>
    </div>

    <h2>06 â€” A Thought to Sit With</h2>
    <div class="prose-callout">
      <span class="prose-callout-label">ğŸ’¬ For You to Consider</span>
      <p>If a model is trained on historical data, it learns historical biases. A hiring model trained on past successful employees might learn to favour candidates who resemble those past employees â€” quietly encoding discrimination into mathematics.</p>
      <p style="margin-top:0.75rem"><strong>Who is responsible for the pipeline's fairness â€” the data collector, the engineer, or the business deploying it?</strong></p>
    </div>

    <h2>07 â€” Coming Up Next</h2>
    <p>Tomorrow â€” <strong>Day 3: How Models Actually Learn.</strong> We open the black box of training itself: what a loss function is, how gradient descent works (the engine behind every model in existence), and why the learning rate is the most consequential number you'll ever tune.</p>
    <div class="prose-callout accent" style="margin-top:1rem">
      <span class="prose-callout-label">ğŸ‘€ Sneak Peek</span>
      <p>Imagine you're blindfolded on a hilly landscape, trying to reach the lowest valley. You can only feel the slope under your feet â€” so you take a small step in whichever direction feels downhill, over and over. That's gradient descent. Tomorrow you'll understand exactly why this blind, incremental process is how every neural network, every language model, and every recommendation system gets smarter.</p>
    </div>

    <h2>08 â€” Go Deeper</h2>
    <p>â–¶ <a href="https://www.youtube.com/watch?v=6kEGUCrBEU0" target="_blank" rel="noopener">IBM: What is a Data Pipeline?</a><br>
    <em style="font-size:0.875rem;color:var(--text-muted)">YouTube Â· 9 min Â· A clear walkthrough of the full pipeline â€” great starting point, no prior knowledge needed.</em></p>
    <p style="margin-top:0.85rem">â–¶ <a href="https://www.kaggle.com/code/imoore/intro-to-exploratory-data-analysis-eda-in-python" target="_blank" rel="noopener">Kaggle: Intro to EDA in Python</a><br>
    <em style="font-size:0.875rem;color:var(--text-muted)">Interactive notebook Â· Run it live in your browser Â· Covers exactly the EDA stage from this article â€” missing values, distributions, correlations, and outliers with real code.</em></p>
    <p style="margin-top:0.85rem">â–¶ <a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank" rel="noopener">scikit-learn: Data Preprocessing Guide</a><br>
    <em style="font-size:0.875rem;color:var(--text-muted)">Official docs Â· Hands-on Â· A detailed reference for the cleaning and feature engineering stages â€” scaling, encoding, and handling missing data.</em></p>
    <p style="margin-top:0.85rem">â–¶ <a href="https://www.kaggle.com/learn/intro-to-machine-learning" target="_blank" rel="noopener">Kaggle: Intro to Machine Learning</a><br>
    <em style="font-size:0.875rem;color:var(--text-muted)">Free course Â· Write and run real code in your browser Â· Covers the full pipeline end-to-end and pairs well with this series.</em></p>

    <div class="prose-callout">
      <span class="prose-callout-label">ğŸ¯ Interview Questions</span>
      <ol>
        <li><strong>What is data leakage and why is it dangerous?</strong><br><em>Data leakage occurs when information from outside the training set improperly influences the model â€” making evaluation results look better than real-world performance will be. It produces models that appear excellent in testing but fail in production.</em></li>
        <li><strong>Why do we need three splits (train / validation / test) instead of two?</strong><br><em>The validation set is used for model selection and hyperparameter tuning. Each tuning decision based on it leaks a small amount of information. The test set is kept completely separate to give a final, unbiased performance estimate that no prior decision has touched.</em></li>
        <li><strong>Why must the scaler be fit only on training data?</strong><br><em>Fitting on the full dataset lets the scaler see test data statistics (mean, standard deviation), which is data leakage. The test set must be treated as completely unseen future data â€” as if it arrived after the model was already trained.</em></li>
        <li><strong>What is EDA and why does it matter?</strong><br><em>Exploratory Data Analysis is the process of visualising and summarising data to understand its structure, spot anomalies, and uncover patterns before modelling. Skipping EDA means building models on data you don't actually understand.</em></li>
        <li><strong>What is the difference between a model parameter and a hyperparameter?</strong><br><em>Parameters are learned automatically from training data â€” the weights in a neural network or the coefficients in a regression. Hyperparameters are configuration choices set by the engineer before training begins, such as learning rate, tree depth, or number of neighbours. The model learns parameters; you tune hyperparameters.</em></li>
      </ol>
    </div>

    <div class="prose-callout">
      <span class="prose-callout-label">ğŸ“ Attachments &amp; Further Reading</span>
      <p style="color:var(--text-muted);font-style:italic;font-size:0.9rem">No attachments for this topic yet.</p>
    </div>

    <div style="margin-top:2.5rem;padding-top:1.5rem;border-top:1px solid var(--border);display:flex;flex-wrap:wrap;gap:0.45rem">
      <span class="tag">#MLPipeline</span><span class="tag">#DataScience</span><span class="tag">#FeatureEngineering</span><span class="tag">#ModelTraining</span><span class="tag">#Deployment</span><span class="tag">#Foundations</span><span class="tag">#30DaysOfAI</span><span class="tag">#Ramadan2026</span>
    </div>
  </div>
</article>

<div class="post-footer-nav">
  <a href="intro-to-ml.html" class="post-nav-card">
    <span class="post-nav-label">â† Day 1</span>
    <span class="post-nav-title">What is Machine Learning?</span>
  </a>
  <a href="how-models-learn.html" class="post-nav-card next">
    <span class="post-nav-label"> Day 3 â†’</span>
    <span class="post-nav-title">How models actually learn</span>
  </a>
</div>

<footer class="footer">
  <div class="footer-inner">
    <div class="footer-main">
      <div>
        <a href="../index.html" class="footer-logo" aria-label="Home"><img src="../assets/logo.png" alt="Mohamed Abdalkader logo" /></a>
        <p class="footer-tagline">AI Engineer Â· Building tomorrow's systems today.</p>
      </div>
      <nav class="footer-nav" aria-label="Footer navigation">
        <a href="../index.html">Home</a>
        <a href="../projects.html">Projects</a>
        <a href="../blog.html">Blog</a>
        <a href="../index.html#contact">Contact</a>
      </nav>
      <div class="social-links">
        <a href="https://github.com/Mo-Abdalkader" class="social-link" target="_blank" rel="noopener">âŒ¥ GitHub</a>
        <a href="https://linkedin.com/in/mo-abdalkader" class="social-link" target="_blank" rel="noopener">âŠ• LinkedIn</a>
        <a href="https://mail.google.com/mail/?view=cm&fs=1&to=Mohameed.Abdalkadeer@gmail.com" class="social-link">âœ‰ Email</a>
      </div>
    </div>
    <div class="footer-bottom">
      <p class="footer-copy">Â© 2026 Mohamed Abdalkader. All rights reserved.</p>
      <span class="footer-built">Built with â™¥ HTML Â· CSS Â· JS</span>
    </div>
  </div>
</footer>

<button class="back-to-top" id="backToTop" aria-label="Back to top">â†‘</button>
<button class="feedback-trigger" id="feedbackTrigger" aria-label="Open feedback panel"><span class="feedback-trigger-text">Feedback</span></button>
<div class="feedback-overlay" id="feedbackOverlay" aria-hidden="true"></div>
<aside class="feedback-panel" id="feedbackPanel" role="dialog" aria-modal="true" aria-labelledby="feedbackTitle" aria-hidden="true">
  <div class="feedback-header">
    <h2 id="feedbackTitle">Send Feedback</h2>
    <button class="feedback-close" id="feedbackClose" aria-label="Close feedback panel">Ã—</button>
  </div>
  <form class="feedback-form" id="feedbackForm">
    <div class="feedback-field"><label for="feedbackName">Name</label><input type="text" id="feedbackName" placeholder="Anonymous" value="Anonymous" maxlength="100" /></div>
    <div class="feedback-field"><label for="feedbackTopic">Topic</label><select id="feedbackTopic"><option value="General">General</option><option value="Suggestion">Suggestion</option><option value="Bug">Bug</option></select></div>
    <div class="feedback-field"><label for="feedbackPage">Page</label><input type="text" id="feedbackPage" readonly /></div>
    <div class="feedback-field">
      <label for="feedbackMessage">Message <span class="required">*</span></label>
      <textarea id="feedbackMessage" rows="5" placeholder="Share your thoughts..." required maxlength="2000"></textarea>
      <div class="char-count"><span id="charCount">0</span> / 2000</div>
    </div>
    <button type="submit" class="btn btn-primary feedback-submit" id="feedbackSubmit">Send Feedback</button>
    <div class="feedback-status" id="feedbackStatus"></div>
  </form>
</aside>

<script src="../js/main.js"></script>
<script src="../js/feedback.js"></script>
<script>
  function toggleQuiz(btn) {
    var answer = btn.nextElementSibling;
    var isVisible = answer.classList.contains('visible');
    answer.classList.toggle('visible', !isVisible);
    btn.textContent = isVisible ? 'Reveal answer â†“' : 'Hide answer â†‘';
  }
</script>
</body>
</html>